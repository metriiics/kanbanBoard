# Настройка Ollama для AI Ассистента

## Установка Ollama

1. **Скачайте и установите Ollama:**
   - Перейдите на https://ollama.com
   - Скачайте установщик для вашей операционной системы
   - Установите Ollama

2. **Запустите Ollama:**
   - После установки Ollama должна автоматически запуститься
   - По умолчанию она работает на `http://localhost:11434`

3. **Установите модель:**
   ```bash
   # Рекомендуемые модели (выберите одну):
   ollama pull llama3.2        # Легкая и быстрая модель (~2GB)
   ollama pull llama3.1:8b    # Более мощная модель (~4.7GB)
   ollama pull mistral        # Альтернативная модель (~4.1GB)
   ollama pull qwen2.5:7b     # Хорошая модель для русского языка (~4.4GB)
   ```

## Настройка проекта

1. **Добавьте настройки Ollama в `.env` файл бэкенда:**
   
   Создайте или отредактируйте файл `backend/.env` и добавьте следующие переменные:
   ```env
   # Ollama настройки (опционально, есть значения по умолчанию)
   OLLAMA_BASE_URL=http://localhost:11434
   OLLAMA_MODEL=llama3.2
   ```
   
   Если эти переменные не указаны, будут использоваться значения по умолчанию:
   - `OLLAMA_BASE_URL=http://localhost:11434`
   - `OLLAMA_MODEL=llama3.2`

2. **Проверьте работу Ollama:**
   ```bash
   # Проверьте, что Ollama запущена
   curl http://localhost:11434/api/tags
   
   # Или протестируйте модель напрямую
   ollama run llama3.2
   ```

## Использование

После настройки AI Ассистент будет автоматически использовать Ollama для ответов на вопросы пользователей.

### API Endpoints

- `POST /api/ai/chat` - Отправка сообщения AI
  ```json
  {
    "message": "Ваш вопрос",
    "model": "llama3.2"  // опционально
  }
  ```

- `GET /api/ai/models` - Получение списка доступных моделей

## Устранение проблем

### Ошибка "Модель не найдена"
- Убедитесь, что модель установлена: `ollama list`
- Установите модель: `ollama pull llama3.2`

### Ошибка "Ollama недоступна"
- Проверьте, что Ollama запущена: `ollama serve`
- Проверьте URL в настройках: `OLLAMA_BASE_URL`

### Медленные ответы
- Используйте более легкую модель (например, `llama3.2` вместо `llama3.1:8b`)
- Убедитесь, что у вас достаточно RAM для выбранной модели

## Рекомендации

- Для разработки используйте легкие модели (`llama3.2`, `mistral:7b`)
- Для продакшена можно использовать более мощные модели (`llama3.1:8b`, `qwen2.5:7b`)
- Модели с поддержкой русского языка (`qwen2.5`, `saiga`) лучше подходят для русскоязычных пользователей

